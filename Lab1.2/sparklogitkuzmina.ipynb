{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8-7dvE_Zm4g"
      },
      "source": [
        "# Как работает логистическая регрессия в Spark: особенности прогноза\n",
        "`Логистическая регрессия` – это статистическая модель, которая используется в машинном обучении для прогнозирования вероятности возникновения некоторого события путем построения логистической функции и сравнения этого события с кривой этой функции. В результате формируется ответ в виде вероятности бинарного события: `0` и `1`, где `0` – событие не произошло, `1` – событие произошло."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI5z3-gpZ5dY"
      },
      "source": [
        "# Работа с логистической регрессией в Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-SV-XNKZ86-"
      },
      "source": [
        "Для того, чтобы начать работу по прогнозу данных, необходимо настроить базовую конфигурацию, импортировав некоторые классы библиотек `Spark MLlib` и `Spark SQL`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahxmpdr9WKfO",
        "outputId": "3349c4f5-b59b-49ef-90fe-dc1b36dc9ff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GdxKYFiibmR",
        "outputId": "33036ccb-b066-4c85-d73e-b87c6f88327d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JRmnDQnjaEEF"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "from pyspark.sql.functions import UserDefinedFunction\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzuSEEL0aJaO"
      },
      "source": [
        "ДАТАСЕТ С ДОМАМИ НА ПРОДАЖУ\n",
        "\n",
        "В качестве примера мы будем использовать датасет Kaggle, который содержит данные о домах на продажу в Бруклине с 2003 по 2017 года и доступен для скачивания. Он содержит 111 атрибутов (столбцов) и 390883 записей (строк). В атрибуты включены: дата продажи, дата постройки, цена на дом, налоговый класс, соседние регионы, долгота, ширина и др."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGNsiOjTb_Hi",
        "outputId": "b03f5e4a-c49f-40d3-92d7-538e4a6192e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-gPSOFi3cqTy"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCbI0XC6c1pP",
        "outputId": "d252f9f2-dbc1-41d5-c4ed-206b9ce46e19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['delivery.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Var11\")\n",
        "os.listdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtNsTpspdJyf"
      },
      "source": [
        "Теперь необходимо импортировать входные данные, создав на их основе набор RDD (Resilient Distributed Dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "C79tyQemiMr6"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "data = spark.read.csv(\n",
        "    '/content/drive/My Drive/Colab Notebooks/Var11/delivery.csv',\n",
        "    inferSchema=True, header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6c22CCGjHOJ"
      },
      "source": [
        "# ГОТОВИМ АТРИБУТ ДЛЯ ПОСЛЕДУЮЩЕЙ БИНАРНОЙ КЛАССИФИКАЦИИ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_4Ey9mWjVYq"
      },
      "source": [
        "Допустим, требуется классифицировать налоговый класс на дом (tax_class). Всего имеется 10 таких классов. Поскольку данные распределены неравномерно (например, в классе 1 имеется 198969 записей, а в 3-м — только 18), мы разделим их на 2 категории: те, которые принадлежат классу 1, и остальные. В Python это делается очень просто, нужно просто вызвать метод replace:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "j6md83Z5dcQH"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Бинаризация статуса\n",
        "data = data.withColumn(\n",
        "    \"status\",\n",
        "    F.when(F.col(\"status\") == \"delayed\", 1).otherwise(0)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfDeelSOdriH"
      },
      "source": [
        "Кроме того, алгоритмы Machine Learning в PySpark работают с числовым значениями, а не со строками. Поэтому преобразуем значения столбца tax_class в тип int:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjuvxzFCjkbz"
      },
      "source": [
        "# ПОДБОР ПРИЗНАКОВ И ПРЕОБРАЗОВАНИЕ КАТЕГОРИЙ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JY6rcIPsdsvH"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "\n",
        "# Индексация для origin\n",
        "indexer_origin = StringIndexer(inputCol=\"origin\", outputCol=\"origin_index\")\n",
        "data = indexer_origin.fit(data).transform(data)\n",
        "\n",
        "# One-Hot Encoding для origin\n",
        "encoder_origin = OneHotEncoder(inputCol=\"origin_index\", outputCol=\"origin_vec\")\n",
        "data = encoder_origin.fit(data).transform(data)\n",
        "\n",
        "# Аналогично для destination\n",
        "indexer_dest = StringIndexer(inputCol=\"destination\", outputCol=\"dest_index\")\n",
        "data = indexer_dest.fit(data).transform(data)\n",
        "\n",
        "encoder_dest = OneHotEncoder(inputCol=\"dest_index\", outputCol=\"dest_vec\")\n",
        "data = encoder_dest.fit(data).transform(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обработка пропусков"
      ],
      "metadata": {
        "id": "PVh3qGrra-Kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка пропусков\n",
        "from pyspark.sql.functions import col, sum as spark_sum\n",
        "\n",
        "data.select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in data.columns]).show()\n",
        "\n",
        "# Импутация (пример для числовых признаков)\n",
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "imputer = Imputer(\n",
        "    inputCols=[\"distance_km\", \"delivery_time_min\"],\n",
        "    outputCols=[\"distance_km_imp\", \"delivery_time_min_imp\"]\n",
        ")\n",
        "data = imputer.fit(data).transform(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njV6BBSZa_oP",
        "outputId": "c4b31b12-c6fd-4cbd-a7a2-1771ad5c2049"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----+------+-----------+-----------+-----------------+------+------------+----------+----------+--------+\n",
            "|delivery_id|date|origin|destination|distance_km|delivery_time_min|status|origin_index|origin_vec|dest_index|dest_vec|\n",
            "+-----------+----+------+-----------+-----------+-----------------+------+------------+----------+----------+--------+\n",
            "|          0|   0|     0|          0|          0|                0|     0|           0|         0|         0|       0|\n",
            "+-----------+----+------+-----------+-----------+-----------------+------+------------+----------+----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__ST1HmjkRcD"
      },
      "source": [
        "# ВЕКТОРИЗАЦИЯ ПРИЗНАКОВ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "id": "AfvqCVuvVsfP"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "\n",
        "# Собираем все признаки\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"distance_km_imp\", \"delivery_time_min_imp\",\n",
        "              \"origin_vec\", \"dest_vec\"],\n",
        "    outputCol=\"raw_features\"\n",
        ")\n",
        "data = assembler.transform(data)\n",
        "\n",
        "# Масштабирование\n",
        "scaler = StandardScaler(\n",
        "    inputCol=\"raw_features\",\n",
        "    outputCol=\"features\",\n",
        "    withStd=True,\n",
        "    withMean=True\n",
        ")\n",
        "data = scaler.fit(data).transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Стратифицирование и разделение"
      ],
      "metadata": {
        "id": "OeSLn75QbXjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ручная стратификация\n",
        "stratified_data = data.stat.sampleBy(\n",
        "    \"status\",\n",
        "    fractions={0: 0.8, 1: 0.8},  # 80% для каждого класса\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Разделение\n",
        "train = stratified_data\n",
        "test = data.subtract(train)"
      ],
      "metadata": {
        "id": "fn7Gi6q3baV5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoLLMchHklZy"
      },
      "source": [
        "Обучение модели с учетом дисбаланса"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7-jD1yy2VsfP"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# 6. Обучение модели с учетом дисбаланса классов\n",
        "# ------------------------------------------------------------\n",
        "# Добавляем веса классов ПЕРЕД разделением на train/test\n",
        "class_weights = {0: 1.0, 1: 5.0}  # Вес для класса 1 (delayed) увеличен в 5 раз\n",
        "\n",
        "data = data.withColumn(\n",
        "    \"class_weight\",\n",
        "    F.when(F.col(\"status\") == 1, class_weights[1]).otherwise(class_weights[0])\n",
        ")\n",
        "\n",
        "# Повторяем стратифицированное разделение после добавления class_weight\n",
        "stratified_data = data.stat.sampleBy(\n",
        "    \"status\",\n",
        "    fractions={0: 0.8, 1: 0.8},\n",
        "    seed=42\n",
        ")\n",
        "train = stratified_data\n",
        "test = data.subtract(train)\n",
        "\n",
        "# Теперь столбец class_weight существует в train\n",
        "lr = LogisticRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"status\",\n",
        "    weightCol=\"class_weight\",  # Убедитесь, что имя столбца совпадает\n",
        "    maxIter=10\n",
        ")\n",
        "\n",
        "model = lr.fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDTFtqSPlDzU"
      },
      "source": [
        "# ДОБАВЛЕНИЕ ПРИЗНАКОВ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peyMWHZVlIdd"
      },
      "source": [
        "Векторизуем также год постройки (year_of_sale) и соседние регионы (neighborhood_id). Для этого нужно только в VectorAssembler указать выбранные признаки:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Отношение времени доставки к расстоянию\n",
        "data = data.withColumn(\n",
        "    \"delivery_speed\",\n",
        "    F.col(\"delivery_time_min_imp\") / F.col(\"distance_km_imp\")\n",
        ")\n",
        "\n",
        "# Категоризация расстояния\n",
        "data = data.withColumn(\n",
        "    \"distance_category\",\n",
        "    F.when(F.col(\"distance_km_imp\") < 500, \"short\")\n",
        "     .when(F.col(\"distance_km_imp\").between(500, 1000), \"medium\")\n",
        "     .otherwise(\"long\")\n",
        ")\n",
        "\n",
        "# Индексация новой категории\n",
        "indexer_dist = StringIndexer(inputCol=\"distance_category\", outputCol=\"dist_cat_index\")\n",
        "data = indexer_dist.fit(data).transform(data)"
      ],
      "metadata": {
        "id": "5j3OTLcncDv6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Оценка модели"
      ],
      "metadata": {
        "id": "nxjny8mYcLhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Посмотрим на распределение предсказаний\n",
        "predictions.groupBy(\"prediction\").count().show()\n",
        "\n",
        "# Выведем несколько примеров с реальными метками\n",
        "predictions.select(\"status\", \"probability\", \"prediction\").filter(F.col(\"status\") == 1).show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U3DAqKLcMm1",
        "outputId": "64ea0564-95d5-4dcf-bd4e-05bdf5e49e41"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|prediction|count|\n",
            "+----------+-----+\n",
            "|       0.0|   17|\n",
            "+----------+-----+\n",
            "\n",
            "+------+-----------+----------+\n",
            "|status|probability|prediction|\n",
            "+------+-----------+----------+\n",
            "+------+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}